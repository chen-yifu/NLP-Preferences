{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Populate_DB (new).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpHLSGEBxWsc",
        "colab_type": "text"
      },
      "source": [
        "Last update: Apr 14\n",
        "\n",
        "# INSTRUCTIONS\n",
        "\n",
        "\n",
        "\n",
        "1.   Upload the \".json\" file (from ABSApp output)\n",
        "2.   In the top menu bar, click \"Runtime\" -> \"Run all\"\n",
        "3.   Wait for the script to finish\n",
        "4.   Refresh the left panel\n",
        "5.   Download the output file\n",
        "6.   Open the \"SQL_inserts.txt\" file\n",
        "7.   Copy the SQL insert statements into MySQL Workbench\n",
        "8.   Use \"Ctrl + A\" to select all, and run all by clicking \"⚡️\"\n",
        "9.   Wait until your database is populated\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGmwdjKxgSVI",
        "colab_type": "text"
      },
      "source": [
        "Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mavh_sZgTJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json, re, datetime\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9E14NozwryR",
        "colab_type": "text"
      },
      "source": [
        "Helper functions for generting SQL insert statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbLygdF8q-BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_ASBA_output = '/content/drive/My Drive/NLP Preferences/forum_data_and_scrapers/ABSApp Output Data/charles_data_mar26.json'\n",
        "path_to_treatment_names = '/content/drive/My Drive/NLP Preferences/database_related/populate_database/approved_drug_names.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpKrVFLzwwsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INPUT: string ; author's flair\n",
        "# OUTPUT: tuple ; (gender, age)\n",
        "def parse_flair(flair):\n",
        "  # NOTE:\n",
        "  # - idea: use disease/drug names when parsing their flair\n",
        "  # FOR REDDIT r/MultipleSclerosis\n",
        "  flair = \" \" + flair + \" \"\n",
        "  # gender\n",
        "  gender = re.findall(\"[^A-Za-z]+([MFmf])[^A-Za-z]+\", flair)\n",
        "  if len(gender):\n",
        "    gender = gender[0]\n",
        "  else:\n",
        "    gender = \"unknown\"\n",
        "  # age\n",
        "  age = re.findall(\"([0-9]{2})[MFmf]|[MFmf]([0-9]{2})|[^0-9']+([2-9][0-9])[^0-9]+\", flair) # assume all users are over age 19\n",
        "  if age:\n",
        "    for t in age[0]:\n",
        "      if t:\n",
        "        age = t\n",
        "        break\n",
        "  else:\n",
        "    age = 0\n",
        "  diagnosis_date = re.findall(\"Dx.*[0-9]{4} | Dx.*[1-2][0-9].[1-2][0-9]\",flair)\n",
        "  return (gender, age)\n",
        "\n",
        "\n",
        "\n",
        "# INPUT: string ; text | int ; index of aspect | string ; mode of detection\n",
        "# OUTPUT: array list ; treatments in DrugBank that occured in this string\n",
        "def detect_treatment(txt, aspect_pos, mode):\n",
        "    res = \"\"\n",
        "    treatment_name_list = []\n",
        "    treatment_df = pd.read_csv(path_to_treatment_names, header=None)\n",
        "      for treatment_name in treatment_df[treatment_df.columns[0]]:\n",
        "          treatment_name_list.append(treatment_name)\n",
        "    if mode not in ['thread title','same sentence','same post','1 sentence-length before','2 sentence-length before']:\n",
        "      raise Exception('{} is not a valid mode for detect_treatment'.format(mode))\n",
        "    treatment_and_pos = {} # format is {treatment_name: position in text}\n",
        "    for t in ['ocrevus', 'copaxone', 'tysabri', 'tecfidera', 'gilenya', 'aubagio', 'baclofen','glatiramer']: # TODO: populate list of drugs using DrugBank.ca data\n",
        "      match = re.search(t, txt.lower(), re.IGNORECASE) # TODO: allows multiple treatments in describes relation\n",
        "      if match != None:\n",
        "        treatment_and_pos[t.title()] = match.span()[1]\n",
        "      # TODO: refactor duplicate logic\n",
        "    if len(treatment_and_pos) == 0:\n",
        "      return \"\"\n",
        "    if mode in ['thread title','same sentence','same post']:\n",
        "      if len(treatment_and_pos) > 1:\n",
        "        print('detected more than 1 treatments for {} and the mode was {}'.format(txt, mode))\n",
        "      # return the last treatment name before aspect index\n",
        "    closest_treatment_and_dictance_to_aspect = (\"\",100000)\n",
        "    for name, treatment_pos in treatment_and_pos.items():\n",
        "      if treatment_pos > aspect_pos and mode in ['1 sentence-length before','2 sentence-length before']: continue # ignore this treatment if it occured after the aspect for certain modes\n",
        "      if abs(aspect_pos - treatment_pos) < closest_treatment_and_dictance_to_aspect[1]:\n",
        "        closest_treatment_and_dictance_to_aspect = (name, aspect_pos - treatment_pos)\n",
        "        res = name\n",
        "    return res\n",
        "\n",
        "\n",
        "# INPUT: string ; text.   string ; forum name\n",
        "# OUTPUT: the detected disease for forum\n",
        "def detect_disease(body, forum):\n",
        "  forum_disease_dict = {\n",
        "      'r/MultipleSclerosis' : 'multiple sclerosis'\n",
        "  }\n",
        "  return forum_disease_dict[forum]\n",
        "\n",
        "# INPUT: string; a row of data\n",
        "# OUTPUT: string ; author SQL insert\n",
        "def gen_author(row):\n",
        "  res = ''\n",
        "  name = row['author']\n",
        "  forum_name = row['forum']\n",
        "  flair = str(row['flair'])\n",
        "  gender, age = parse_flair(str(flair))\n",
        "  res = res + 'INSERT IGNORE INTO author (author_username, author_gender, author_age, author_flair, author_profile_url, author_forum_name) VALUES (\\\"' + name + '\\\", \\\"'+ gender + '\\\", '+ str(age) + ',\\\"'+ flair +'\\\",\"' + 'http://reddit.com/user/'+ name + '\",\"{}\");'.format(forum_name)+  '\\n'\n",
        "  return res\n",
        "\n",
        "\n",
        "\n",
        "# OUTPUT: string ; disease SQL insert\n",
        "# NOTE:   needs to be hand-coded\n",
        "def gen_disease():\n",
        "  # following statistics from https://subredditstats.com/r/multiplesclerosis\n",
        "  res = 'INSERT IGNORE INTO disease (disease_name) VALUES (\"multiple sclerosis\");'\n",
        "  return res\n",
        "\n",
        "\n",
        "# OUTPUT: string ; forum SQL insert\n",
        "# NOTE: needs to be hand-coded\n",
        "def gen_forum():\n",
        "  # following statistics from https://subredditstats.com/r/multiplesclerosis\n",
        "  res = \"\"\"INSERT IGNORE INTO forum (forum_name, forum_url, forum_disease_name, forum_user_count, forum_activity_count) VALUES ('r/MultipleSclerosis', 'https://www.reddit.com/r/MultipleSclerosis/', 'multiple sclerosis', 15537, 23504);\\n\"\"\"\n",
        "  return res\n",
        "\n",
        "def gen_category():\n",
        "  res = \"INSERT IGNORE INTO category (category_name) VALUES ('temporary category');\"\n",
        "  return res\n",
        "\n",
        "\n",
        "# INPUT:  string ; a row of data\n",
        "# OUTPUT: string ; post SQL insert statement\n",
        "def gen_post(row):\n",
        "  res = ''\n",
        "  post_id = row['id']\n",
        "  body = row['body']\n",
        "  body = re.sub('\"',\"'\",body)\n",
        "  timestamp = int(row['time'])\n",
        "  timestamp = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S') \n",
        "  score = row['score']\n",
        "  author = row['author']\n",
        "  forum_name = row['forum']\n",
        "  disease_name = row['disease']\n",
        "  sub_title = re.sub('\"',\"'\",row['sub_title'])\n",
        "  reply_to_id = row['reply_to_id']\n",
        "  size = len(body)\n",
        "  res = res + 'INSERT IGNORE INTO post (post_id, post_size, post_timestamp, post_body_hash, post_forum_name, post_disease_name, post_score, post_author_username, post_thread_title, post_body, post_reply_to_post_id) VALUES (\\\"'  \\\n",
        "  + str(post_id) + '\",' \\\n",
        "  + str(size) + ',\"' \\\n",
        "  + str(timestamp) + '\\\", MD5(\\\"' \\\n",
        "  + body + '\\\")' \\\n",
        "  + ', \"{}\"'.format(forum_name)  \\\n",
        "  + ', \"{}\"'.format(disease_name) \\\n",
        "  + ',' + str(score) \\\n",
        "  + ',' + '\"{}\"'.format(author)\\\n",
        "  + ',\"' + sub_title + '\"' \\\n",
        "  + ',\"' + body \\\n",
        "  + '\",' + reply_to_id \\\n",
        "  + ');' + \"\\n\" \n",
        "  return res\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# INPUT: string ; a row of data\n",
        "# OUTPUT: string ; sentence SQL insert statement\n",
        "def gen_sentence(row):\n",
        "  res = ''\n",
        "  try: \n",
        "    sentences = eval(row['inference'])['_sentences']\n",
        "  except:\n",
        "    return \"\"\n",
        "  timestamp = int(row['time'])\n",
        "  timestamp = datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S') \n",
        "  body = row['body']\n",
        "  body = re.sub('\"',\"'\",body)\n",
        "  for s in sentences:\n",
        "      sent = body[s['_start']:s['_end']]\n",
        "      temp = 'INSERT IGNORE INTO sentence(sentence_body, sentence_post_id, sentence_body_hash) VALUES (\\\"' + sent + '\\\"' + ', (SELECT post_id FROM post WHERE post_body_hash = MD5(\\\"' + body +'\\\") and post_timestamp = \"{}\"), MD5(\\\"'.format(str(timestamp)) + sent +'\\\"));'\n",
        "      res = res + temp + \"\\n \\n\"\n",
        "  return res\n",
        "\n",
        "\n",
        "\n",
        "# OUTPUT: string ; treatment SQL insert statement\n",
        "def gen_treatment():\n",
        "  res = \"\"\n",
        "  treatment_df = pd.read_csv(path_to_treatment_names, header=None)\n",
        "  for treatment_name in treatment_df[treatment_df.columns[0]]:\n",
        "    res += 'INSERT IGNORE INTO treatment(treatment_name) VALUES (\"{}\");\\n'.format(treatment_name)\n",
        "  return res + 'INSERT IGNORE INTO treatment(treatment_name) VALUES (\"\");\\n'\n",
        "#   return \"\"\"INSERT IGNORE INTO treatment(treatment_name) VALUES (\"Ocrevus\");\\n\n",
        "# INSERT IGNORE INTO treatment(treatment_name) VALUES (\"Copaxone\");\\n\n",
        "# INSERT IGNORE INTO treatment(treatment_name) VALUES (\"Tysabri\");\\n\n",
        "# INSERT IGNORE INTO treatment(treatment_name) VALUES (\"Tecfidera\");\\n\n",
        "# INSERT IGNORE INTO treatment(treatment_name) VALUES (\"Gilenya\");\\n\n",
        "# INSERT IGNORE INTO treatment(treatment_name) VALUES (\"Aubagio\");\\n\n",
        "# INSERT IGNORE INTO treatment(treatment_name) VALUES (\"Baclofen\");\\n\n",
        "# INSERT IGNORE INTO treatment(treatment_name) VALUES (\"Glatiramer\");\\n\n",
        "# INSERT IGNORE INTO treatment(treatment_name) VALUES (\"None\");\\n\"\"\"\n",
        "\n",
        "\n",
        "# INPUT: string ; a row of data\n",
        "# OUTPUT: array list of tuples ; describes insert statements\n",
        "def gen_describes(row):\n",
        "  res = []\n",
        "  try: \n",
        "      sentences = eval(row['inference'])['_sentences']\n",
        "  except:\n",
        "    return \"\"\n",
        "  body = row['body']\n",
        "  body = re.sub('\"',\"'\",body)\n",
        "  post_id = row['id']\n",
        "  forum_name = row['forum']\n",
        "  author_username = row['author']\n",
        "  time = int(row['time'])\n",
        "  time = datetime.utcfromtimestamp(time).strftime('%Y-%m-%d %H:%M:%S') \n",
        "  treatment_thread_title = detect_treatment(row['sub_title'],0,'thread title')\n",
        "  for i,s in enumerate(sentences):\n",
        "      sentence_body = body[s['_start']:s['_end']]\n",
        "      # Get treatment field\n",
        "      for e in s['_events']:\n",
        "        aspect_location = e[0]['_start']\n",
        "        same_sentence_treatment = detect_treatment(sentence_body, aspect_location - s['_start'],'same sentence') # second argument is the relative position of aspect to sentence\n",
        "        same_post_treatment = detect_treatment(body, aspect_location, 'same post')\n",
        "        treatment_within_110_char_upwards = detect_treatment(body[max(0,int(e[0]['_start'])-110):e[0]['_start']], aspect_location - max(0,int(e[0]['_start'])-110), '1 sentence-length before')\n",
        "        treatment_within_220_char_upwards = detect_treatment(body[max(0,int(e[0]['_start'])-220):e[0]['_start']], aspect_location - max(0,int(e[0]['_start'])-220), '2 sentence-length before')\n",
        "        for word in e:\n",
        "          # Get aspect field\n",
        "          if word['_type'] == \"ASPECT\":\n",
        "            aspect = word['_text']\n",
        "            aspect = re.sub('\"',\"'\",aspect)\n",
        "          # Get opinion field\n",
        "          if word['_type'] == \"OPINION\":\n",
        "            opi = word['_text']\n",
        "            opi = re.sub('\"',\"'\",opi.strip())\n",
        "            score = word['_score']\n",
        "            opi_start = word['_start']\n",
        "            opi_end = opi_start + word['_len']\n",
        "        temp_opi = 'INSERT IGNORE INTO opinion_word(opinion_word_name, opinion_word_polarity) values (\\\"' + opi.strip() + '\\\", ' + str(score) + ');'\n",
        "        temp_aspect = 'INSERT IGNORE INTO aspect(aspect_name, aspect_category_name) values (\\\"' + aspect + '\\\",\"temporary category\");'\n",
        "        temp_describes = 'INSERT IGNORE INTO describes(describes_aspect_name, describes_opinion_word_name, describes_sentence_id, describes_same_sentence_treatment_name, describes_same_post_treatment_name, describes_110_characters_treatment_name, describes_220_characters_treatment_name, describes_thread_title_treatment_name, describes_forum_name, describes_author_username, describes_timestamp) VALUES (' \\\n",
        "        + '\"{}\"'.format(aspect) + ', ' \\\n",
        "        + '\"{}\"'.format(opi.strip()) + ',' \\\n",
        "        + '(SELECT sentence_id FROM sentence WHERE sentence_body_hash = MD5(\\\"' \\\n",
        "        + sentence_body + '\\\")),'.format(post_id)\\\n",
        "        + \"'{}','{}', '{}', '{}','{}','{}','{}','{}'\".format(same_sentence_treatment, same_post_treatment,treatment_within_110_char_upwards,treatment_within_220_char_upwards, treatment_thread_title, forum_name, author_username, time) + ');' \n",
        "        res.append((temp_opi,temp_aspect,temp_describes))\n",
        "  return res\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVgtSQJGAd1j",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8X8mF8ewqR5",
        "colab_type": "text"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDghVhM5wg7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OUTPUT: pd.DataFrame ; stores JSON data in dataframe, simplifies key\n",
        "def load_data(filepath):\n",
        "  with open(filepath, 'r+') as file:\n",
        "      content = file.read()\n",
        "      file.seek(0)\n",
        "      content.replace('\"', \"'\")\n",
        "      file.write(content)\n",
        "\n",
        "  with open(filepath, 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  # column names in new dataframe\n",
        "  column_headers = [\n",
        "             'id',                  # this post's ID\n",
        "             'author',              # author username\n",
        "             'flair',               # author flair\n",
        "             'body',                # post/comment content text\n",
        "             'inference',           # inference for body text\n",
        "             'time',                # timestamp\n",
        "             'score',               # score\n",
        "             'thread_id',           # generated thread_id\n",
        "             'position_in_thread',  # position in thread, 1 for post, 2 for first comment in thread, etc\n",
        "             'is_post?',            # True if post, False if comment\n",
        "             'forum',               # forum name\n",
        "             'disease',             # disease name\n",
        "             'sub_title',           # thread title\n",
        "             'reply_to_id'          # the post id that this post replies to\n",
        "             ]\n",
        "  # data placeholder for dataframe\n",
        "  flat_data = []\n",
        "  # remove emojis?\n",
        "  # emoji_pattern = re.compile(\"[\"\n",
        "  #       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "  #       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "  #       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "  #       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "  #                          \"]+\", flags=re.UNICODE)\n",
        "  for i, sub in enumerate(data):\n",
        "    sub_row = [] # current row\n",
        "    sub_row.append(sub['sub_id'])\n",
        "    sub_row.append(sub['sub_author'])\n",
        "    sub_row.append(sub['sub_author_flair'])\n",
        "    sub_row.append(sub['sub_body'])\n",
        "    try:\n",
        "      sub_row.append(sub['sub_body_inference'])\n",
        "    except KeyError:\n",
        "      sub_row.append({})\n",
        "    sub_row.append(sub['sub_time'])\n",
        "    sub_row.append(sub['sub_score'])\n",
        "    sub_row.append(i)\n",
        "    sub_row.append(1)\n",
        "    sub_row.append(True)\n",
        "    sub_row.append('r/MultipleSclerosis') # TODO Change this to be a mutable variable\n",
        "    sub_row.append(detect_disease(sub['sub_body'],'r/MultipleSclerosis')) # TODO change second parameter of detect_disease, forum\n",
        "    sub_row.append(sub['sub_title'])\n",
        "    sub_row.append(\"Null\")\n",
        "    flat_data.append(sub_row)\n",
        "    assert(len(sub_row) == len(column_headers))\n",
        "\n",
        "    # iterate through comments and add to flat_data\n",
        "    for k, com in enumerate(sub['comments']):\n",
        "      cur_row = []\n",
        "      cur_row.append(com['comment_id'])\n",
        "      cur_row.append(com['comment_author'])\n",
        "      cur_row.append(com['comment_author_flair'])\n",
        "      cur_row.append(com['comment_body'])\n",
        "      try:\n",
        "        cur_row.append(com['comment_body_inference'])\n",
        "      except KeyError:\n",
        "        cur_row.append({})\n",
        "      cur_row.append(com['comment_time'])\n",
        "      cur_row.append(com['comment_score'])\n",
        "      cur_row.append(i)\n",
        "      cur_row.append(k+2)\n",
        "      cur_row.append(False)\n",
        "      cur_row.append('r/MultipleSclerosis') # TODO Change this to be a mutable variable\n",
        "      cur_row.append(detect_disease(com['comment_body'],'r/MultipleSclerosis')) # TODO change second parameter of detect_disease, forum\n",
        "      cur_row.append('')\n",
        "      cur_row.append('\"'+com['comment_parent_id'][3:]+'\"') # the JSON file contains prefix (e.g. 't3_')  for all comments\n",
        "      flat_data.append(cur_row)\n",
        "      assert(len(cur_row) == len(column_headers))\n",
        "      \n",
        "  assert(i==len(data)-1)\n",
        "  df = pd.DataFrame(flat_data, columns=column_headers)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LriR46QwxHS",
        "colab_type": "text"
      },
      "source": [
        "Iterate through using helpers to generate insert statements in SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdAFL-BQgHWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_all(df):\n",
        "  # create dataframe for storing raw JSON data\n",
        "  # create dataframe for SQL insert statements\n",
        "  post_and_sentence_SQL = \"\"\n",
        "  # iterate through \n",
        "  forum_SQL = gen_forum()\n",
        "  disease_SQL = gen_disease()\n",
        "  treatment_SQL = gen_treatment()\n",
        "  category_SQL = gen_category()\n",
        "  author_SQL = []\n",
        "  aspect_SQL = []\n",
        "  opinion_SQL = []\n",
        "  describes_SQL = []\n",
        "  for i, row in df.iterrows():\n",
        "    author_SQL.append(gen_author(row))\n",
        "    post_SQL = gen_post(row)\n",
        "    sentence_SQL = gen_sentence(row)\n",
        "    temp = gen_describes(row)\n",
        "    for tupl in temp:\n",
        "      aspect_SQL.append(tupl[0])\n",
        "      opinion_SQL.append(tupl[1])\n",
        "      describes_SQL.append(tupl[2])\n",
        "    # print(\"{}/{} rows processed, {}% complete\".format(i,len(df),round(100*i/len(df),1)))\n",
        "    post_and_sentence_SQL = post_and_sentence_SQL + \"\\n\" + post_SQL + \"\\n\" + sentence_SQL # + \"\\n\" + describes_SQL + \"\\n\"\n",
        "\n",
        "  # preserve only unique authors, aspects, and opinion_words\n",
        "  author_SQL = \"\\n\".join(list(set(author_SQL)))\n",
        "  aspect_SQL = \"\\n\".join(list(set(aspect_SQL)))\n",
        "  opinion_SQL = \"\\n\".join(list(set(opinion_SQL)))\n",
        "  describes_SQL = \"\\n\".join(describes_SQL) # do not remove describes that \"look\" the same\n",
        "  print(\"-------------\\nall SQL statements have been successfully generated\")\n",
        "\n",
        "  return \"\\n\".join([disease_SQL, treatment_SQL, forum_SQL, author_SQL, category_SQL, post_SQL, post_and_sentence_SQL, aspect_SQL, opinion_SQL, describes_SQL])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tchLJx8Zw1LE",
        "colab_type": "text"
      },
      "source": [
        "Output SQL statements into text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCY070fkNEHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def export_SQL(all_SQL):\n",
        "  file_name = r\"SQL_inserts_\"+str(datetime.now())[:10] + \"_\"+ str(datetime.now())[11:19]+ \".txt\"\n",
        "  SQL_file = open(file_name,'w')\n",
        "  try:\n",
        "    SQL_file.write(all_SQL)\n",
        "    SQL_file.write(\"\")\n",
        "  except:\n",
        "    print(\"An error occured.\")\n",
        "  print(\"\"\"All SQL insert statements have been saved to '{}', please download it from the left panel. \\n\n",
        "  You might need to refresh the 'Files' panel (by clicking the button next to 'upload'), but NOT the whole page.\"\"\".format(file_name))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8EYLNphbytE",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDVCgLdyU6bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  # data is flattened and assigned to df\n",
        "  df = load_data(path_to_ASBA_output) # path to Google Drive.  For path to example data: /content/drive/My Drive/NLP/charles_example_feb14.json\n",
        "\n",
        "  # generate insert statements\n",
        "  all_SQL = \"use mydb; \\n\" + gen_all(df) # the database name is 'mydb'\n",
        "\n",
        "\n",
        "  # export file\n",
        "  export_SQL(all_SQL)\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-zvmQVGYC0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}